{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "the_office.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from types import SimpleNamespace\n",
        "from collections import Counter\n",
        "import os\n",
        "import re\n",
        "import pathlib\n",
        "import array\n",
        "import pickle\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import math"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2022-03-22T14:47:46.275144Z",
          "iopub.execute_input": "2022-03-22T14:47:46.275476Z",
          "iopub.status.idle": "2022-03-22T14:47:46.282030Z",
          "shell.execute_reply.started": "2022-03-22T14:47:46.275437Z",
          "shell.execute_reply": "2022-03-22T14:47:46.281208Z"
        },
        "trusted": true,
        "id": "1ottVuubKern"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = open('dataset.txt', 'r')\n",
        "readfile = f.readlines()\n",
        "dataset = [x for x in readfile if '\\n' != x or '<EOE>' != x]\n",
        "dataset = np.array(dataset)\n",
        "print(dataset)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-22T14:47:46.333629Z",
          "iopub.execute_input": "2022-03-22T14:47:46.333864Z",
          "iopub.status.idle": "2022-03-22T14:47:46.492133Z",
          "shell.execute_reply.started": "2022-03-22T14:47:46.333836Z",
          "shell.execute_reply": "2022-03-22T14:47:46.491395Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGPFe82uKers",
        "outputId": "f5b6de04-632f-4995-a1fe-2e702eafdf56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Michael: All right Jim. Your quarterlies look very good. How are things at the library?\\n'\n",
            " '\\n' \"Jim: Oh, I told you. I couldn't close it. So...\\n\" ...\n",
            " \"Pam: I thought it was weird when you picked us to make a documentary. But all in all...I think an ordinary paper company like Dunder Mifflin was a great subject for a documentary. There's a lot of beauty in ordinary things. Isn't that kind of the point?\\n\"\n",
            " '\\n' '<EOE>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.shape"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-22T14:47:46.493747Z",
          "iopub.execute_input": "2022-03-22T14:47:46.494477Z",
          "iopub.status.idle": "2022-03-22T14:47:46.500119Z",
          "shell.execute_reply.started": "2022-03-22T14:47:46.494441Z",
          "shell.execute_reply": "2022-03-22T14:47:46.499382Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0HWcYHaKert",
        "outputId": "8a33f6d7-e936-4b6c-aa95-56a72d7d9954"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(110460,)"
            ]
          },
          "metadata": {},
          "execution_count": 383
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(dataset):\n",
        "  \"\"\"\n",
        "  :param dataset: array of sentences delimited by /n.\n",
        "  :return: array of sentences with the symbols tokenized.\n",
        "  \"\"\"\n",
        "  punct={\n",
        "          '.':\" ||period|| \",\n",
        "          \",\":\" ||comma|| \",\n",
        "          '\"':\" ||doublequotationmark|| \",\n",
        "          \"'\":\" ||quotationmark|| \",\n",
        "          \"?\":\" ||questionmark|| \",\n",
        "          \"-\":\" ||dash|| \",\n",
        "          \"\\n\":\" ||return||\",\n",
        "          \")\":\" ||rightparentheses|| \",\n",
        "          \"(\":\" ||leftparentheses|| \",\n",
        "          \"[\":\" ||leftbracket|| \",\n",
        "          \"]\":\" ||rightbracket|| \",\n",
        "          \";\":\" ||semicolon|| \",\n",
        "          \"!\":\" ||exclamationmark|| \",\n",
        "          \":\": \" ||colon|| \",\n",
        "          \"%\":\" ||percent|| \",\n",
        "          \"$\":\" ||dollar|| \",\n",
        "          \"#\":\" ||hashtag|| \",\n",
        "          \"/\":\" ||forwardbar|| \"\n",
        "      }\n",
        "\n",
        "  new_dataset = []\n",
        "  for sentence in dataset:\n",
        "    sentence = sentence.replace('.', punct['.'])\n",
        "    sentence = sentence.replace(',', punct[','])\n",
        "    sentence = sentence.replace('\"', punct['\"'])\n",
        "    sentence = sentence.replace(\"'\", punct[\"'\"])\n",
        "    sentence = sentence.replace('?', punct['?'])\n",
        "    sentence = sentence.replace('-', punct['-'])\n",
        "    sentence = sentence.replace('\\n', punct['\\n'])\n",
        "    sentence = sentence.replace(')', punct[')'])\n",
        "    sentence = sentence.replace('(', punct['('])\n",
        "    sentence = sentence.replace('[', punct['['])\n",
        "    sentence = sentence.replace(']', punct[']'])\n",
        "    sentence = sentence.replace(';', punct[';'])\n",
        "    sentence = sentence.replace('!', punct['!'])\n",
        "    sentence = sentence.replace(':', punct[':'])\n",
        "    sentence = sentence.replace('%', punct['%'])\n",
        "    sentence = sentence.replace('$', punct['$'])\n",
        "    sentence = sentence.replace('#', punct['#'])\n",
        "    sentence = sentence.replace('/', punct['/'])\n",
        "    new_dataset += sentence.split(\" \")\n",
        "  return np.array(new_dataset)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-22T14:47:46.501467Z",
          "iopub.execute_input": "2022-03-22T14:47:46.502066Z",
          "iopub.status.idle": "2022-03-22T14:47:46.517848Z",
          "shell.execute_reply.started": "2022-03-22T14:47:46.502028Z",
          "shell.execute_reply": "2022-03-22T14:47:46.516995Z"
        },
        "trusted": true,
        "id": "dds89VKBKeru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vocab(text):\n",
        "  \"\"\"\n",
        "  :param text: text split into sentences delimited by /n.\n",
        "  :return: an np array with unique strings, the vocabulary of the dataset.\n",
        "  \"\"\"\n",
        "  vocab = np.unique(text)\n",
        "  return vocab"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-22T14:47:46.559597Z",
          "iopub.execute_input": "2022-03-22T14:47:46.560006Z",
          "iopub.status.idle": "2022-03-22T14:47:46.567416Z",
          "shell.execute_reply.started": "2022-03-22T14:47:46.559970Z",
          "shell.execute_reply": "2022-03-22T14:47:46.566711Z"
        },
        "trusted": true,
        "id": "e9LXHI61Kerv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_lookup_tables(text):\n",
        "    \"\"\"\n",
        "    Create lookup tables for vocabulary\n",
        "    :param text: The text of tv scripts split into words\n",
        "    :return: A tuple of dicts (vocab_to_int, int_to_vocab)\n",
        "    \"\"\"\n",
        "    word_counts=Counter(text)\n",
        "    sorted_vocab=sorted(word_counts,key=word_counts.get,reverse=True)\n",
        "\n",
        "    texti=set(text)\n",
        "    vocab_to_int={word:ii for ii,word in enumerate(texti,0)}\n",
        "    int_to_vocab={items:key for key,items in vocab_to_int.items()}\n",
        "\n",
        "    return (vocab_to_int, int_to_vocab)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-22T14:47:46.618375Z",
          "iopub.execute_input": "2022-03-22T14:47:46.619664Z",
          "iopub.status.idle": "2022-03-22T14:47:46.626378Z",
          "shell.execute_reply.started": "2022-03-22T14:47:46.619626Z",
          "shell.execute_reply": "2022-03-22T14:47:46.625411Z"
        },
        "trusted": true,
        "id": "Y3Nx4CmkKerw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_int(text, vocab_to_int):\n",
        "  s = pd.Series(text)\n",
        "  s = s.map(vocab_to_int)\n",
        "  return s.to_numpy()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-22T14:47:46.691035Z",
          "iopub.execute_input": "2022-03-22T14:47:46.691272Z",
          "iopub.status.idle": "2022-03-22T14:47:46.695708Z",
          "shell.execute_reply.started": "2022-03-22T14:47:46.691242Z",
          "shell.execute_reply": "2022-03-22T14:47:46.695031Z"
        },
        "trusted": true,
        "id": "bN6sLdsfKerw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def int_to_text(int_text, int_to_vocab):\n",
        "  s = pd.Series(int_text)\n",
        "  s = s.map(int_to_vocab)\n",
        "  return s.to_numpy()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-22T14:47:46.759108Z",
          "iopub.execute_input": "2022-03-22T14:47:46.759319Z",
          "iopub.status.idle": "2022-03-22T14:47:46.763735Z",
          "shell.execute_reply.started": "2022-03-22T14:47:46.759294Z",
          "shell.execute_reply": "2022-03-22T14:47:46.762853Z"
        },
        "trusted": true,
        "id": "ZhNK1MxlKery"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reduce_mem_usage(train_data):\n",
        "    col_type = train_data[0].dtype\n",
        "\n",
        "    if col_type != object:\n",
        "        c_min = train_data.min()\n",
        "        c_max = train_data.max()\n",
        "        if str(col_type)[:3] == 'int':\n",
        "            if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                train_data = train_data.astype(np.int8)\n",
        "            elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                train_data = train_data.astype(np.int16)\n",
        "            elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                train_data = train_data.astype(np.int32)\n",
        "            elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                train_data = train_data.astype(np.int64)  \n",
        "        else:\n",
        "            if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                train_data = train_data.astype(np.float16)\n",
        "            elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                train_data = train_data.astype(np.float32)\n",
        "            else:\n",
        "                train_data = train_data.astype(np.float64)\n",
        "    else:\n",
        "        train_data = train_data.astype('category')\n",
        "    print(train_data[0].dtype)\n",
        "    return train_data"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-22T14:47:46.812959Z",
          "iopub.execute_input": "2022-03-22T14:47:46.813365Z",
          "iopub.status.idle": "2022-03-22T14:47:46.823068Z",
          "shell.execute_reply.started": "2022-03-22T14:47:46.813332Z",
          "shell.execute_reply": "2022-03-22T14:47:46.822198Z"
        },
        "trusted": true,
        "id": "LikY7n7EKery"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_text = tokenize(dataset)\n",
        "vocabulary = vocab(tokenized_text)\n",
        "vocab_to_int, int_to_vocab = create_lookup_tables(vocabulary)\n",
        "int_text = text_to_int(tokenized_text, vocab_to_int)\n",
        "#int_text = reduce_mem_usage(int_text)\n",
        "\n",
        "del dataset\n",
        "del tokenized_text"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-22T14:47:46.867066Z",
          "iopub.execute_input": "2022-03-22T14:47:46.867248Z",
          "iopub.status.idle": "2022-03-22T14:47:47.780508Z",
          "shell.execute_reply.started": "2022-03-22T14:47:46.867226Z",
          "shell.execute_reply": "2022-03-22T14:47:47.779788Z"
        },
        "trusted": true,
        "id": "0szaZgt_Kerz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(int_text.shape)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-22T14:47:47.781972Z",
          "iopub.execute_input": "2022-03-22T14:47:47.782780Z",
          "iopub.status.idle": "2022-03-22T14:47:47.789537Z",
          "shell.execute_reply.started": "2022-03-22T14:47:47.782741Z",
          "shell.execute_reply": "2022-03-22T14:47:47.788654Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOXxOL24Ker0",
        "outputId": "ac5156ae-d765-4640-d651-a8c626caae34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1400061,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "    print(\"WARNING: Training without GPU can be very slow!\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-22T14:47:47.791133Z",
          "iopub.execute_input": "2022-03-22T14:47:47.791559Z",
          "iopub.status.idle": "2022-03-22T14:47:47.796792Z",
          "shell.execute_reply.started": "2022-03-22T14:47:47.791461Z",
          "shell.execute_reply": "2022-03-22T14:47:47.795974Z"
        },
        "trusted": true,
        "id": "MYOSTQM5Ker0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "def batch_data(words, sequence_length, batch_size):\n",
        "    \"\"\"\n",
        "    Batch the neural network data using DataLoader\n",
        "    :param words: The word ids of the TV scripts\n",
        "    :param sequence_length: The sequence length of each batch\n",
        "    :param batch_size: The size of each batch; the number of sequences in a batch\n",
        "    :return: DataLoader with batched data\n",
        "    \"\"\"\n",
        "    feature_tensors = []\n",
        "    target_tensors = []\n",
        "    for i in range(len(words)):\n",
        "      target_idx = i + sequence_length\n",
        "      if target_idx < len(words):\n",
        "        features = words[i:i + sequence_length]\n",
        "        feature_tensors.append(features)\n",
        "\n",
        "        target = words[target_idx]\n",
        "        target_tensors.append(target)\n",
        "        \n",
        "    data = TensorDataset(torch.from_numpy(np.array(feature_tensors)).to(device), torch.from_numpy(np.array(target_tensors)).to(device))\n",
        "    data_loader = torch.utils.data.DataLoader(data, shuffle=False, batch_size=batch_size)\n",
        "    # return a dataloader\n",
        "      \n",
        "    return data_loader"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-22T14:47:47.799162Z",
          "iopub.execute_input": "2022-03-22T14:47:47.799631Z",
          "iopub.status.idle": "2022-03-22T14:47:47.807476Z",
          "shell.execute_reply.started": "2022-03-22T14:47:47.799595Z",
          "shell.execute_reply": "2022-03-22T14:47:47.806440Z"
        },
        "trusted": true,
        "id": "2BloWTfsKer1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def attention(query, key, value, mask=None, dropout=None):\n",
        "    \"Compute 'Scaled Dot Product Attention'\"\n",
        "    d_k = query.size(-1)\n",
        "    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
        "             / math.sqrt(d_k)\n",
        "    p_attn = F.softmax(scores, dim = -1)\n",
        "    if dropout is not None:\n",
        "        p_attn = dropout(p_attn)\n",
        "    return torch.matmul(p_attn, value), p_attn"
      ],
      "metadata": {
        "id": "AzrWFqBSKsY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' Data params '''\n",
        "# Sequence Length\n",
        "sequence_length = 12   # of words in a sequence\n",
        "# Batch Size\n",
        "batch_size = 400\n",
        "\n",
        "# data loader - do not change\n",
        "train_loader = batch_data(int_text, sequence_length, batch_size)\n",
        "\n",
        "'''Training parameters'''\n",
        "# Number of Epochs\n",
        "num_epochs = 4\n",
        "# Learning Rate\n",
        "learning_rate = 0.0008\n",
        "\n",
        "'''Model parameters'''\n",
        "# Vocab size\n",
        "vocab_size = len(vocabulary)\n",
        "# Output size\n",
        "output_size = vocab_size\n",
        "# Embedding Dimension\n",
        "embedding_dim = 200\n",
        "\n",
        "# Show stats for every n number of batches\n",
        "show_every_n_batches = 20\n",
        "print(vocab_size)\n",
        "print(len(train_loader))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-22T14:47:47.837645Z",
          "iopub.execute_input": "2022-03-22T14:47:47.837891Z",
          "iopub.status.idle": "2022-03-22T14:47:48.648381Z",
          "shell.execute_reply.started": "2022-03-22T14:47:47.837860Z",
          "shell.execute_reply": "2022-03-22T14:47:48.647589Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7ol1dCmKer2",
        "outputId": "b932f6b2-9363-4fe2-de40-02bc1755eaf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24531\n",
            "3501\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, bias=True):\n",
        "        super().__init__()\n",
        "        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
        "        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
        "        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
        "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        # Empirically observed the convergence to be much better with the scaled initialization\n",
        "        nn.init.xavier_uniform_(self.k_proj.weight, gain=1 / math.sqrt(2))\n",
        "        nn.init.xavier_uniform_(self.v_proj.weight, gain=1 / math.sqrt(2))\n",
        "        nn.init.xavier_uniform_(self.q_proj.weight, gain=1 / math.sqrt(2))\n",
        "        nn.init.xavier_uniform_(self.out_proj.weight)\n",
        "        if self.out_proj.bias is not None:\n",
        "            nn.init.constant_(self.out_proj.bias, 0.)\n",
        "\n",
        "    # B = Batch size\n",
        "    # W = Number of context words (left + right)\n",
        "    # E = embedding_dim\n",
        "    def forward(self, x):\n",
        "        # x shape is (B, W, E)\n",
        "        q = self.q_proj(x)\n",
        "        # q shape is (B, W, E)\n",
        "        k = self.k_proj(x)\n",
        "        # k shape is (B, W, E)\n",
        "        v = self.v_proj(x)\n",
        "        # k shape is (B, W, E)\n",
        "        y, _ = attention(q, k, v)\n",
        "        # y shape is (B, W, E)\n",
        "        y = self.out_proj(y)\n",
        "        # y shape is (B, W, E)\n",
        "        return y\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-22T14:47:47.808900Z",
          "iopub.execute_input": "2022-03-22T14:47:47.809179Z",
          "iopub.status.idle": "2022-03-22T14:47:47.824824Z",
          "shell.execute_reply.started": "2022-03-22T14:47:47.809145Z",
          "shell.execute_reply": "2022-03-22T14:47:47.823866Z"
        },
        "trusted": true,
        "id": "3nHLU5zwKer1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerLayer(nn.Module):\n",
        "    def __init__(self, d_model, dim_feedforward=512, dropout=0.1, activation=\"relu\"):\n",
        "        super().__init__()\n",
        "        self.self_attn1 = SelfAttention(d_model)\n",
        "        self.self_attn2 = SelfAttention(d_model)\n",
        "        self.self_attn3 = SelfAttention(d_model)\n",
        "\n",
        "        # Implementation of Feedforward model\n",
        "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src):\n",
        "        multi_headed = np.array_split(src.cpu().detach().numpy(), 3)\n",
        "\n",
        "        m_src1 = self.self_attn1(torch.from_numpy(multi_headed[0]).to(device))\n",
        "        m_src2 = self.self_attn2(torch.from_numpy(multi_headed[1]).to(device))\n",
        "        m_src3 = self.self_attn3(torch.from_numpy(multi_headed[2]).to(device))\n",
        "\n",
        "        src2 = torch.cat((m_src1, m_src2, m_src3))\n",
        "        src = src + self.dropout1(src2)\n",
        "        src = self.norm1(src)\n",
        "        src2 = self.linear2(self.dropout(F.relu(self.linear1(src))))\n",
        "        src = src + self.dropout2(src2)\n",
        "        src = self.norm2(src)\n",
        "        return src"
      ],
      "metadata": {
        "id": "KbYlsn3VK8wa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Predictor(nn.Module):\n",
        "    def __init__(self, num_embeddings, embedding_dim, context_words=sequence_length):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(num_embeddings, embedding_dim, padding_idx=0)\n",
        "        self.lin = nn.Linear(embedding_dim, num_embeddings, bias=False)\n",
        "        self.att = TransformerLayer(embedding_dim)\n",
        "        self.position_embedding = nn.Parameter(torch.Tensor(context_words, embedding_dim))\n",
        "        nn.init.xavier_uniform_(self.position_embedding)\n",
        "\n",
        "        self.lin2 = nn.Linear(embedding_dim, num_embeddings, bias=False)\n",
        "        self.att2 = TransformerLayer(embedding_dim)\n",
        "\n",
        "    # B = Batch size\n",
        "    # W = Number of context words (left + right)\n",
        "    # E = embedding_dim\n",
        "    # V = num_embeddings (number of words)\n",
        "    def forward(self, input):\n",
        "        # input shape is (B, W)\n",
        "        e = self.emb(input)\n",
        "        # e shape is (B, W, E)\n",
        "        u = e + self.position_embedding\n",
        "        # u shape is (B, W, E)\n",
        "\n",
        "        v = self.att(u)\n",
        "        # v shape is (B, W, E)\n",
        "        x = v.sum(dim=1)\n",
        "        # x shape is (B, E)\n",
        "        y = self.lin(x)\n",
        "\n",
        "        # y shape is (B, V)\n",
        "        return y\n"
      ],
      "metadata": {
        "id": "qz5v7s9FLCE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Predictor(len(vocabulary), embedding_dim).to(device)"
      ],
      "metadata": {
        "id": "utRJ4TdkLF4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, criterion, optimizer, n_epochs, device, show_every_n_batches=100):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    ncorrect = 0\n",
        "    ntokens = 0\n",
        "    niterations = 0\n",
        "    for batch_i, (inputs, labels) in enumerate(train_loader, 1):\n",
        "      n_batches = len(train_loader.dataset)//batch_size\n",
        "      if(batch_i > n_batches):\n",
        "        break\n",
        "      model.zero_grad()\n",
        "\n",
        "      output = model(inputs)\n",
        "      loss = criterion(output, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      # Training statistics\n",
        "      total_loss += loss.item()\n",
        "      ncorrect += (torch.max(output, 1)[1] == labels).sum().item()\n",
        "      ntokens += labels.numel()\n",
        "      niterations += 1\n",
        "      if niterations == 200 or niterations == 500 or niterations % 1000 == 0:\n",
        "          print(f'Train: wpb={ntokens//niterations}, num_updates={niterations}, accuracy={100*ncorrect/ntokens:.1f}, loss={total_loss/ntokens:.2f}')\n",
        "\n",
        "    total_loss = total_loss / ntokens\n",
        "    accuracy = 100 * ncorrect / ntokens\n",
        "\n",
        "    return accuracy, total_loss"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-22T14:47:48.650596Z",
          "iopub.execute_input": "2022-03-22T14:47:48.651039Z",
          "iopub.status.idle": "2022-03-22T14:47:48.659417Z",
          "shell.execute_reply.started": "2022-03-22T14:47:48.651000Z",
          "shell.execute_reply": "2022-03-22T14:47:48.658696Z"
        },
        "trusted": true,
        "id": "EnwnPOwwKer3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss(reduction='sum')\n",
        "\n",
        "train_accuracy = []\n",
        "wiki_accuracy = []\n",
        "valid_accuracy = []\n",
        "for epoch_i in range(1, num_epochs + 1):\n",
        "    acc, loss = train(model, criterion, optimizer, device, show_every_n_batches)\n",
        "    train_accuracy.append(acc)\n",
        "    print(f'| epoch {epoch_i:03d} | train accuracy={acc:.1f}%, train loss={loss:.2f}')\n",
        "\n",
        "# Save model\n",
        "torch.save(model.state_dict(), 'the_office_scripts')\n",
        "print('Model Trained and Saved')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lI21MUmhLpO6",
        "outputId": "1b6628be-0e5e-46bf-f00b-916d971fc6b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: wpb=400, num_updates=200, accuracy=27.5, loss=5.51\n",
            "Train: wpb=400, num_updates=500, accuracy=34.2, loss=4.71\n",
            "Train: wpb=400, num_updates=1000, accuracy=38.9, loss=4.21\n",
            "Train: wpb=400, num_updates=2000, accuracy=42.1, loss=3.83\n",
            "Train: wpb=400, num_updates=3000, accuracy=43.3, loss=3.68\n",
            "| epoch 001 | train accuracy=43.8%, train loss=3.64\n",
            "Train: wpb=400, num_updates=200, accuracy=49.1, loss=2.96\n",
            "Train: wpb=400, num_updates=500, accuracy=48.6, loss=2.97\n",
            "Train: wpb=400, num_updates=1000, accuracy=48.5, loss=2.98\n",
            "Train: wpb=400, num_updates=2000, accuracy=48.2, loss=2.97\n",
            "Train: wpb=400, num_updates=3000, accuracy=47.9, loss=3.00\n",
            "| epoch 002 | train accuracy=47.8%, train loss=3.00\n",
            "Train: wpb=400, num_updates=200, accuracy=50.3, loss=2.76\n",
            "Train: wpb=400, num_updates=500, accuracy=49.7, loss=2.76\n",
            "Train: wpb=400, num_updates=1000, accuracy=49.5, loss=2.77\n",
            "Train: wpb=400, num_updates=2000, accuracy=49.1, loss=2.77\n",
            "Train: wpb=400, num_updates=3000, accuracy=48.7, loss=2.80\n",
            "| epoch 003 | train accuracy=48.6%, train loss=2.81\n",
            "Train: wpb=400, num_updates=200, accuracy=50.8, loss=2.62\n",
            "Train: wpb=400, num_updates=500, accuracy=50.3, loss=2.62\n",
            "Train: wpb=400, num_updates=1000, accuracy=50.1, loss=2.63\n",
            "Train: wpb=400, num_updates=2000, accuracy=49.7, loss=2.64\n",
            "Train: wpb=400, num_updates=3000, accuracy=49.3, loss=2.67\n",
            "| epoch 004 | train accuracy=49.2%, train loss=2.68\n",
            "Model Trained and Saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def generate(model, prime_id, int_to_vocab, token_dict, pad_value, predict_len=100):\n",
        "    \"\"\"\n",
        "    Generate text using the neural network\n",
        "    :param decoder: The PyTorch Module that holds the trained neural network\n",
        "    :param prime_id: The word id to start the first prediction\n",
        "    :param int_to_vocab: Dict of word id keys to word values\n",
        "    :param token_dict: Dict of puncuation tokens keys to puncuation values\n",
        "    :param pad_value: The value used to pad a sequence\n",
        "    :param predict_len: The length of text to generate\n",
        "    :return: The generated text\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    # create a sequence (batch_size=1) with the prime_id\n",
        "    current_seq = np.full((1, sequence_length), pad_value)\n",
        "    current_seq[-1][-1] = prime_id\n",
        "    predicted = [int_to_vocab[prime_id]]\n",
        "    \n",
        "    for _ in range(predict_len):\n",
        "        \n",
        "        \n",
        "        # get the output of the rnn\n",
        "        current_seq= torch.tensor(current_seq).to(device)\n",
        "        output = model(current_seq)\n",
        "\n",
        "        # get the next word probabilities\n",
        "        p = F.softmax(output, dim=1).data\n",
        "\n",
        "        # use top_k sampling to get the index of the next word\n",
        "        top_k = 5\n",
        "        p, top_i = p.topk(top_k)\n",
        "        top_i = top_i.cpu().detach().numpy().squeeze()\n",
        "\n",
        "        # select the likely next word index with some element of randomness\n",
        "        p = p.cpu().detach().numpy().squeeze()\n",
        "        word_i = np.random.choice(top_i, p=p/p.sum())\n",
        "\n",
        "        # retrieve that word from the dictionary\n",
        "        word = int_to_vocab[word_i]\n",
        "        predicted.append(word)     \n",
        "        \n",
        "        # the generated word becomes the next \"current sequence\" and the cycle can continue\n",
        "        current_seq = np.roll(current_seq.cpu().detach().numpy(), -1, 1)\n",
        "        current_seq[-1][-1] = word_i\n",
        "    \n",
        "    gen_sentences = ' '.join(predicted)\n",
        "    \n",
        "    # Replace punctuation tokens\n",
        "    for key, token in token_dict.items():\n",
        "        ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
        "        gen_sentences = gen_sentences.replace(' ' + token.lower(), key)\n",
        "    gen_sentences = gen_sentences.replace('\\n ', '\\n')\n",
        "    gen_sentences = gen_sentences.replace('( ', '(')\n",
        "    gen_sentences = gen_sentences.replace('  ', ' ')\n",
        "    \n",
        "    # return all the sentences\n",
        "    return gen_sentences"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-22T14:58:12.428398Z",
          "iopub.execute_input": "2022-03-22T14:58:12.429116Z",
          "iopub.status.idle": "2022-03-22T14:58:12.441409Z",
          "shell.execute_reply.started": "2022-03-22T14:58:12.429075Z",
          "shell.execute_reply": "2022-03-22T14:58:12.440569Z"
        },
        "trusted": true,
        "id": "geYLlRbqKer4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen_length = 500 # modify the length to your preference\n",
        "prime_word = 'Michael' # name for starting the script\n",
        "\n",
        "token_dict= {\n",
        "          '.':\"||period||\",\n",
        "          \",\":\"||comma||\",\n",
        "          '\"':\"||doublequotationmark||\",\n",
        "          \"'\":\"||quotationmark||\",\n",
        "          \"?\":\"||questionmark||\",\n",
        "          \"-\":\"||dash||\",\n",
        "          \"\\n\":\"||return||\",\n",
        "          \")\":\"||rightparentheses||\",\n",
        "          \"(\":\"||leftparentheses||\",\n",
        "          \"[\":\"||leftbracket||\",\n",
        "          \"]\":\"||rightbracket||\",\n",
        "          \";\":\"||semicolon||\",\n",
        "          \"!\":\"||exclamationmark||\",\n",
        "          \":\": \"||colon||\",\n",
        "          \"%\":\"||percent||\",\n",
        "          \"$\":\"||dollar||\",\n",
        "          \"#\":\"||hashtag||\",\n",
        "          \"/\":\"||forwardbar||\"\n",
        "      }\n",
        "\n",
        "pad_word= ''\n",
        "generated_script = generate(model, vocab_to_int[prime_word], int_to_vocab, token_dict, vocab_to_int[pad_word], gen_length)\n",
        "print(generated_script)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-22T14:58:12.442583Z",
          "iopub.execute_input": "2022-03-22T14:58:12.443158Z",
          "iopub.status.idle": "2022-03-22T14:58:15.778791Z",
          "shell.execute_reply.started": "2022-03-22T14:58:12.443122Z",
          "shell.execute_reply": "2022-03-22T14:58:15.777961Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqKh0ZjlKer4",
        "outputId": "d14c398f-88a2-4256-b525-f6e749ba0544"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Michael, I' d have to go to see it. I can do it for the altar. \n",
            "\n",
            "Pam: [ whispering] [ to hug] \n",
            "\n",
            "Kevin: [ laughing] I' m sorry. I' m gonna be happy. \n",
            "\n",
            "Dwight: I' m sorry, I' m gonna go ahead! \n",
            "\n",
            "Jim: I know what I' d say. \n",
            "\n",
            "Jim: I don' t know. \n",
            "\n",
            "Andy: [ dancing] ! \n",
            "\n",
            "Jim: Oh, I' m sorry, it' s really really really really. I' d love you to do anything but, and then I' ve been here. \n",
            "\n",
            "Dwight: Okay, well, it' s like a little dehydrated. It' s really good. \n",
            "\n",
            "Pam: [ dancing] I love your bride. \n",
            "\n",
            "Dwight: It' s just fine . . . . . . um. . . . . . . it' s Athleap. . . . . . . it was really nice. It' s like a fairy tale. It' s just like a fairy tale. [ applause] Oh, well, well, you' re gonna get it for me to the altar and I didn' t know. I just feel better. I can' t do anything. \n",
            "\n",
            "Jim: [ to hug] Oh my second? \n",
            "\n",
            "Dwight: No, no. \n",
            "\n",
            "Jim: I don' t want to see this. \n",
            "\n",
            "Dwight: [ laughs] I don' t wanna go to leave this. \n",
            "\n",
            "Jim: Oh, no, no. It was just so you had to be happy. \n",
            "\n",
            "Dwight: [ laughs] \n",
            "\n",
            "Pam: Okay, well well, well, I don' t know that I' m gonna have to go with this. \n",
            "\n",
            "Jim: Okay, it' s just. I' ll get your attention with Drake\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "hyKzA0g_DmMI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}